{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from vit_3d import ViT\n",
    "from mae import MAE\n",
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "import yaml\n",
    "import time\n",
    "import cv2\n",
    "import h5py\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "from collections import OrderedDict\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics import f1_score, average_precision_score, roc_auc_score\n",
    "from monai.utils import first, set_determinism\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    EnsureChannelFirstd,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    SaveImaged,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    Invertd,\n",
    ")\n",
    "import multiprocessing as mp\n",
    "from einops import rearrange\n",
    "from monai import transforms\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.networks.nets import UNet\n",
    "from monai.networks.layers import Norm\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.data import CacheDataset, DataLoader, Dataset, decollate_batch\n",
    "from monai.config import print_config\n",
    "from monai.apps import download_and_extract\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import shutil\n",
    "#导入ddp\n",
    "\n",
    "import torch.distributed\n",
    "import torch.multiprocessing as mp\n",
    "import argparse\n",
    "from attrdict import AttrDict\n",
    "\n",
    "import waterz\n",
    "# import evaluate as ev\n",
    "from skimage.metrics import adapted_rand_error as adapted_rand_ref\n",
    "from skimage.metrics import variation_of_information as voi_ref\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#logging \n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "em_FAFB_dir = sorted(glob('/data/cyd0806/EM_raw_image/hdf_data/nii.gz/*.gz'))\n",
    "em_data_dicts = [\n",
    "    {\"image\": image_name}\n",
    "    for image_name in em_FAFB_dir\n",
    "]\n",
    "#实例化tensorboard\n",
    "record_dir = '/output/logs'\n",
    "save_dir = '/data/cyd0806/EM_raw_image/MAE/DecisionMAE_32*160*160'\n",
    "if not os.path.exists(record_dir):\n",
    "    os.makedirs(record_dir)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "writer = SummaryWriter(log_dir= record_dir)\n",
    "#logging记录em_FAFB_dir的长度\n",
    "logger.info(f\"em_FAFB_dir length: {len(em_FAFB_dir)}\")\n",
    "#tensorboard记录em_FAFB_dir的长度\n",
    "\n",
    "with open('/data/ydchen/VLP/bigmodel/IJCAI23/MAE/config/pretraining_all.yaml', 'r') as f:\n",
    "        cfg = AttrDict(yaml.safe_load(f))\n",
    "\n",
    "# Random apply\n",
    "class RandomApply(nn.Module):\n",
    "    def __init__(self, fn, p=0.3):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        if random.random() > self.p:\n",
    "            return x\n",
    "        return self.fn(x)\n",
    "\n",
    "# 1*84*2048*2048\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\"]),\n",
    "        EnsureChannelFirstd(keys=[\"image\"]),\n",
    "        # ScaleIntensityRanged(\n",
    "        #     keys=[\"image\"], a_min=100, a_max=255,\n",
    "        #     b_min=0.0, b_max=1.0, clip=True,\n",
    "        # ),\n",
    "        CropForegroundd(keys=[\"image\"], source_key=\"image\"),\n",
    "        RandomApply(Orientationd(keys=[\"image\"], axcodes=\"RAS\")),\n",
    "        # transforms.RandAffined(\n",
    "        #     keys=['image'],\n",
    "        #     mode=('bilinear', 'nearest'),\n",
    "        #     shear_range=(0.5, 0.5, 0.5),\n",
    "        #     prob=1.0, spatial_size=(128,128,48),\n",
    "        #     rotate_range=(0, np.pi/15),\n",
    "        #    ),\n",
    "        \n",
    "        # 随机噪声\n",
    "        transforms.RandGaussianNoised(keys=[\"image\"], prob=0.1, mean=0.0, std=0.1),\n",
    "        transforms.RandSpatialCropSamplesd(\n",
    "            keys=[\"image\"],\n",
    "            roi_size=[160,160,32],\n",
    "            random_size=False,\n",
    "            num_samples=6,\n",
    "        ),\n",
    "        transforms.ScaleIntensityRangeD(\n",
    "            keys=[\"image\"], a_min=0.0, a_max=255.0, b_min=0.0, b_max=1.0, clip=True)\n",
    "\n",
    "        # transforms.ScaleIntensityd(keys=[\"image\"]),\n",
    "        # user can also add other random transforms\n",
    "        \n",
    "        #transforms.ToTensord(keys=[\"image\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model = ViT(\n",
    "                        image_size = 160,          # image size\n",
    "                        frames = 32,               # number of frames\n",
    "                        image_patch_size = 16,     # image patch size\n",
    "                        frame_patch_size = 4,      # frame patch size\n",
    "                        channels=1,\n",
    "                        num_classes = 1000,\n",
    "                        dim = 768,\n",
    "                        depth = 12,\n",
    "                        heads = 12,\n",
    "                        mlp_dim = 3072,\n",
    "                        dropout = 0.1,\n",
    "                        emb_dropout = 0.1\n",
    "                    )\n",
    "\n",
    "mae = MAE(\n",
    "    encoder = model,\n",
    "    masking_ratio = 0.5,   # the paper recommended 75% masked patches\n",
    "    decoder_dim = 512,      # paper showed good results with just 512\n",
    "    decoder_depth = 6,       # anywhere from 1 to 8\n",
    "    hog = False\n",
    ")\n",
    "# 定义优化器\n",
    "lr = 1e-5\n",
    "\n",
    "def calculate_lr(iters):\n",
    "    if iters < 3000:\n",
    "        current_lr = (lr - lr/10) * pow(1 - float(iters - 300) / 3000, cfg.TRAIN.power) + cfg.TRAIN.end_lr\n",
    "    else:\n",
    "        current_lr = lr/10\n",
    "    return current_lr\n",
    "    \n",
    "optimizer = torch.optim.Adam(mae.parameters(), lr=lr, weight_decay=0.00001, amsgrad=True,betas=(0.9, 0.999))\n",
    "#optimizer = torch.optim.SGD(mae.parameters(), lr=lr,momentum=0.9,weight_decay=0.00001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_weight = '/data/cyd0806/EM_raw_image/MAE/DecisionMAE_32*160*160/mae_full_mae_32*160*160_760.pth'\n",
    "mae.load_state_dict(torch.load(mae_weight),strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "for i,batch_data in enumerate(train_loader):\n",
    "    batch_data['image'] = rearrange(batch_data['image'], 'b c x y z -> b c z x y')\n",
    "    images = batch_data['image'].to(device)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange,repeat\n",
    "device = torch.device('cuda:0')\n",
    "img = torch.randn(4,1,32,160,160).to(device)\n",
    "# get patches\n",
    "mae = mae.to(device)\n",
    "img = img.to(device)\n",
    "patches = mae.to_patch(img)\n",
    "print(f'patches shape: {patches.shape}')\n",
    "batch, num_patches, *_ = patches.shape\n",
    "\n",
    "# patch to encoder tokens and add positions\n",
    "\n",
    "tokens = mae.patch_to_emb(patches)\n",
    "print(f'tokens shape: {tokens.shape}')\n",
    "tokens = tokens + mae.encoder.pos_embedding[:, 1:(num_patches + 1)]\n",
    "print(f'tokens shape: {tokens.shape}')\n",
    "# calculate of patches needed to be masked, and get random indices, dividing it up for mask vs unmasked\n",
    "\n",
    "num_masked = int(mae.masking_ratio * num_patches)\n",
    "#rand_indices = mae.decision_net(patches).argsort(dim = -1)\n",
    "rand_indices = torch.rand(batch, num_patches, device = device).argsort(dim = -1)\n",
    "print(f'rand indices shape: {rand_indices.shape}')\n",
    "masked_indices, unmasked_indices = rand_indices[:, :num_masked], rand_indices[:, num_masked:]\n",
    "print(f'masked indices shape: {masked_indices.shape}, unmasked indices shape: {unmasked_indices.shape}')\n",
    "# get the unmasked tokens to be encoded\n",
    "\n",
    "batch_range = torch.arange(batch, device = device)[:, None]\n",
    "print(f'batch range shape: {batch_range.shape}')\n",
    "print(f'batch range: {batch_range}')\n",
    "tokens = tokens[batch_range, unmasked_indices]\n",
    "print(f'tokens shape: {tokens.shape}')\n",
    "# get the patches to be masked for the final reconstruction loss\n",
    "\n",
    "masked_patches = patches[batch_range, masked_indices]\n",
    "print(f'masked patches shape: {masked_patches.shape}')\n",
    "if mae.HOG:\n",
    "    masked_patches = rearrange(masked_patches.cpu().numpy(), 'b p (h w d) -> b p h w d', h = mae.encoder.image_patch_size, w = mae.encoder.image_patch_size,d = mae.encoder.frame_patch_size)\n",
    "    for b in range(len(masked_patches)):\n",
    "        for p in range(len(masked_patches[0])):\n",
    "            _, temp_hog = hog(masked_patches[b,p,:,:,2], orientations=8, pixels_per_cell=(4, 4),\n",
    "                    cells_per_block=(1, 1), visualize=True, multichannel=False)\n",
    "            masked_patches[b,p] = repeat(temp_hog, 'h w -> h w d', d = mae.encoder.frame_patch_size)\n",
    "    masked_patches = torch.tensor(rearrange(masked_patches, 'b p h w d -> b p (h w d)')).to(device)\n",
    "# attend with vision transformer\n",
    "\n",
    "encoded_tokens = mae.encoder.transformer(tokens)\n",
    "\n",
    "# project encoder to decoder dimensions, if they are not equal - the paper says you can get away with a smaller dimension for decoder\n",
    "\n",
    "decoder_tokens = mae.enc_to_dec(encoded_tokens)\n",
    "\n",
    "# reapply decoder position embedding to unmasked tokens\n",
    "\n",
    "unmasked_decoder_tokens = decoder_tokens + mae.decoder_pos_emb(unmasked_indices)\n",
    "\n",
    "# repeat mask tokens for number of masked, and add the positions using the masked indices derived above\n",
    "\n",
    "mask_tokens = repeat(mae.mask_token, 'd -> b n d', b = batch, n = num_masked)\n",
    "mask_tokens = mask_tokens + mae.decoder_pos_emb(masked_indices)\n",
    "\n",
    "# concat the masked tokens to the decoder tokens and attend with decoder\n",
    "\n",
    "decoder_tokens = torch.zeros(batch, num_patches, mae.decoder_dim, device=device)\n",
    "decoder_tokens[batch_range, unmasked_indices] = unmasked_decoder_tokens\n",
    "decoder_tokens[batch_range, masked_indices] = mask_tokens\n",
    "decoded_tokens = mae.decoder(decoder_tokens)\n",
    "print(f'decoded tokens shape: {decoded_tokens.shape}')\n",
    "\n",
    "# splice out the mask tokens and project to pixel values\n",
    "\n",
    "mask_tokens = decoded_tokens[batch_range, masked_indices]\n",
    "pred_pixel_values = mae.to_pixels(mask_tokens)\n",
    "print(f'pred pixel values shape: {pred_pixel_values.shape}')\n",
    "pred_pixel_values_sigmoid = torch.sigmoid(pred_pixel_values)\n",
    "        \n",
    "# calculate reconstruction loss\n",
    "\n",
    "recon_loss = F.mse_loss(pred_pixel_values, masked_patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给图像增加高斯模糊\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "\n",
    "def gaussian_blur(img, kernel_size, sigma):\n",
    "    # 高斯模糊\n",
    "    img = cv2.GaussianBlur(img, (kernel_size, kernel_size), sigma)\n",
    "    return img\n",
    "\n",
    "def random_gaussian_blur(img):\n",
    "    # 随机高斯模糊\n",
    "    kernel_size = random.choice([5,7,9])\n",
    "    sigma = random.choice([1, 2, 3])\n",
    "    img = gaussian_blur(img, kernel_size, sigma)\n",
    "    img = img + np.random.randn(*img.shape) * random.choice([0.02, 0.03, 0.05])\n",
    "    return img\n",
    "\n",
    "def random_rotate(img):\n",
    "    # 随机旋转\n",
    "    angle = random.choice([0, 90, 180, 270])\n",
    "    img = np.rot90(img, angle // 90)\n",
    "    return img\n",
    "\n",
    "def random_crop(img, crop_size):\n",
    "    # 随机裁剪\n",
    "    h, w = img.shape[:2]\n",
    "    y = random.randint(0, h - crop_size)\n",
    "    x = random.randint(0, w - crop_size)\n",
    "    img = img[y:y + crop_size, x:x + crop_size]\n",
    "    return img\n",
    "\n",
    "def random_flip(img):\n",
    "    # 随机翻转\n",
    "    flip = random.choice([0, 1, 2])\n",
    "    img = np.flip(img, flip)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_patches = rearrange(masked_patches.cpu().numpy(), 'b p (h w d) -> b p h w d', h = mae.encoder.image_patch_size, w = mae.encoder.image_patch_size,d = mae.encoder.frame_patch_size)\n",
    "for i in range(masked_patches.shape[1]):\n",
    "    for j in range(masked_patches.shape[2]):\n",
    "        masked_patches[0,i,j,:,:] = random_gaussian_blur(masked_patches[0,i,j,:,:])\n",
    "masked_patches = torch.tensor(rearrange(masked_patches, 'b p h w d -> b p (h w d)')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_tokens = torch.zeros(batch, num_patches, 1024, device=device)\n",
    "recons_tokens[batch_range, unmasked_indices] = patches[batch_range, unmasked_indices]\n",
    "recons_tokens[batch_range, masked_indices] = 0\n",
    "patches2 = rearrange(recons_tokens, 'b (f h w) (p1 p2 pf c) -> b c (f pf) (h p1) (w p2)',f = 8,h = 10,w=10,p1 = 16,p2 = 16,pf = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patches2[0,0,0,:,:].cpu().numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i in range(0,32,6):\n",
    "    print(i)\n",
    "    plt.subplot(6,1,i/6+1)\n",
    "    plt.imshow(img[0,0,i].cpu(),cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.savefig('./visual/img_raw_0124.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "for i in range(0,32,6):\n",
    "    print(i)\n",
    "    plt.subplot(6,1,i/6+1)\n",
    "    plt.imshow(patches2[0,0,i].cpu(),cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.savefig('./visual/img_random_0124.png',dpi=300,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 20 30\n",
    "plt.imshow(patches2[0,0,30].cpu().detach().numpy(),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('visual/ours_MAE1.png',bbox_inches='tight',pad_inches=0,dpi=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img[0,0,20].cpu().numpy(),cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.savefig('visual/raw_image3.png',bbox_inches='tight',pad_inches=0,dpi=480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gauss = random_gaussian_blur(patches2[0,0,30].cpu().detach().numpy())\n",
    "plt.imshow(gauss,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_tokens = torch.zeros(batch, num_patches, 1024, device=device)\n",
    "recons_tokens[batch_range, unmasked_indices] = patches[batch_range, unmasked_indices]\n",
    "recons_tokens[batch_range, masked_indices] = masked_patches\n",
    "patches2 = rearrange(recons_tokens, 'b (f h w) (p1 p2 pf c) -> b c (f pf) (h p1) (w p2)',f = 8,h = 10,w=10,p1 = 16,p2 = 16,pf = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(patches2[0,0,30].cpu().detach().numpy(),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_pixel_values = rearrange(pred_pixel_values, 'b p (h w d) -> b p h w d', h = mae.encoder.image_patch_size, w = mae.encoder.image_patch_size,d = mae.encoder.frame_patch_size)\n",
    "masked_patches = rearrange(masked_patches, 'b p (h w d) -> b p h w d', h = mae.encoder.image_patch_size, w = mae.encoder.image_patch_size,d = mae.encoder.frame_patch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(pred_pixel_values[0,0,:,:,1].cpu().detach().numpy(),cmap = 'gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(masked_patches[0,0,:,:,1].cpu().detach().numpy(),cmap = 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 28 07:25:54 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.63.01    Driver Version: 470.63.01    CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          Off  | 00000000:4F:00.0 Off |                    0 |\n",
      "|  0%   29C    P0    70W / 300W |  30089MiB / 45634MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          Off  | 00000000:52:00.0 Off |                    0 |\n",
      "|  0%   31C    P0    81W / 300W |  44447MiB / 45634MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A40          Off  | 00000000:56:00.0 Off |                    0 |\n",
      "|  0%   31C    P0    80W / 300W |  44431MiB / 45634MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A40          Off  | 00000000:57:00.0 Off |                    0 |\n",
      "|  0%   33C    P0    80W / 300W |  44427MiB / 45634MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=b=c=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
